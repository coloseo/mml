<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title></title>
<meta name="author" content="(Ging.wu)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/css/theme/moon.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h2 class="author">Ging.wu</h2><p class="date">Created: 2019-06-18 二 12:24</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-orgbc346a4">1. 全批量梯度下降算法</a>
<ul>
<li><a href="#/slide-orgcb75894">1.1. 建立如下最小的二乘线性回归模型</a></li>
<li><a href="#/slide-org2f5848b">1.2. 最小二乘法的损失函数 \(L(\beta)\) 为</a></li>
<li><a href="#/slide-org77a380d">1.3. 损失函数的 \(L(\beta)\) 的梯度</a></li>
<li><a href="#/slide-org22f2bb3">1.4. 梯度下降法的步骤</a>
<ul>
<li><a href="#/slide-orgbd03f3d">1.4.1. 当 \(i=0\),自己设置初始点 \(\beta^{0}=\left(\beta_{0}^{0}, \beta_{1}^{0}\right)\) ， 设置步长（也就是学习率 \(\alpha\) ), 设置迭代终止的误差忍耐度 \(tol\) 。</a></li>
<li><a href="#/slide-orgb469ee2">1.4.2. 计算代价函数 \(L(\beta)\) 在点 \((\beta_{0}^{i}, \beta_{1}^{i})\) 上的梯度 \(\nabla L_{\beta^{i}}\)</a></li>
<li><a href="#/slide-orgcd73d6c">1.4.3. 计算 \(\beta^{i+1}\) , 公式如下</a></li>
<li><a href="#/slide-orgad69082">1.4.4. 计算梯度 \(\nabla L_{\beta^{i+1}}\) , 如果梯度的二范数 \(\left\|\nabla L_{\beta^{i+1}}\right\|_{2}\) 小于等于 \(tol\) ,则迭代停止，最优解的取值为 \(\beta^{i+1}\) ;如果它大于 \(tol\), 那么 \(i=i+1\) , 并返回第三步。</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#/slide-orgbae7565">2. 随机梯度下降算法</a>
<ul>
<li><a href="#/slide-org37ee54c">2.1. 回顾全批量梯度下降算法如何计算每次迭代中的梯度</a></li>
</ul>
</li>
</ul>
</div>
</div>
</section>
<section>
<section id="slide-sec-1">
<h2 id="orgbc346a4"><span class="section-number-2">1</span> 全批量梯度下降算法</h2>
<div class="outline-text-2" id="text-1">
</div>
</section>
<section id="slide-sec-1-1">
<h3 id="orgcb75894"><span class="section-number-3">1.1</span> 建立如下最小的二乘线性回归模型</h3>
<p>
\(y=\beta_{0}+\beta_{1} x\)
</p>
</section>
<section id="slide-sec-1-2">
<h3 id="org2f5848b"><span class="section-number-3">1.2</span> 最小二乘法的损失函数 \(L(\beta)\) 为</h3>
<p>
\(L(\beta)=\frac{1}{N} \sum_{j=1}^{N}\left(y_{j}-\hat{y}_{j}\right)^{2}=\sum_{j=1}^{N} \frac{1}{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)^{2}\)
</p>
</section>
<section id="slide-sec-1-3">
<h3 id="org77a380d"><span class="section-number-3">1.3</span> 损失函数的 \(L(\beta)\) 的梯度</h3>
<p>
\(\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(\frac{2}{N} \sum_{j=1}^{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right), \frac{2}{N} \sum_{j=1}^{N} x_{j}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)\right)\)
</p>
</section>
<section id="slide-sec-1-4">
<h3 id="org22f2bb3"><span class="section-number-3">1.4</span> 梯度下降法的步骤</h3>
<div class="outline-text-3" id="text-1-4">
</div>
</section>
<section id="slide-sec-1-4-1">
<h4 id="orgbd03f3d"><span class="section-number-4">1.4.1</span> 当 \(i=0\),自己设置初始点 \(\beta^{0}=\left(\beta_{0}^{0}, \beta_{1}^{0}\right)\) ， 设置步长（也就是学习率 \(\alpha\) ), 设置迭代终止的误差忍耐度 \(tol\) 。</h4>
</section>
<section id="slide-sec-1-4-2">
<h4 id="orgb469ee2"><span class="section-number-4">1.4.2</span> 计算代价函数 \(L(\beta)\) 在点 \((\beta_{0}^{i}, \beta_{1}^{i})\) 上的梯度 \(\nabla L_{\beta^{i}}\)</h4>
</section>
<section id="slide-sec-1-4-3">
<h4 id="orgcd73d6c"><span class="section-number-4">1.4.3</span> 计算 \(\beta^{i+1}\) , 公式如下</h4>
<p>
\(\beta^{i+1}=\beta^{i}-\alpha \nabla L_{\beta^{i}}\)
</p>
</section>
<section id="slide-sec-1-4-4">
<h4 id="orgad69082"><span class="section-number-4">1.4.4</span> 计算梯度 \(\nabla L_{\beta^{i+1}}\) , 如果梯度的二范数 \(\left\|\nabla L_{\beta^{i+1}}\right\|_{2}\) 小于等于 \(tol\) ,则迭代停止，最优解的取值为 \(\beta^{i+1}\) ;如果它大于 \(tol\), 那么 \(i=i+1\) , 并返回第三步。</h4>


</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="orgbae7565"><span class="section-number-2">2</span> 随机梯度下降算法</h2>
<p>
  随机梯度下降算法(Stochastic Gradient Decent, SGD) 是对全批量梯度下降算法计算效率的改进算法。本质上来说，我们预期随机梯度下降法得到的
结果和全批量梯度下降法相接近; SGD的优势是更快地计算梯度
</p>
</section>
<section id="slide-sec-2-1">
<h3 id="org37ee54c"><span class="section-number-3">2.1</span> 回顾全批量梯度下降算法如何计算每次迭代中的梯度</h3>
<p>
   \[\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(\frac{2}{N} \sum_{j=1}^{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right), \frac{2}{N} \sum_{j=1}^{N} x_{j}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)\right)
\]
全批量梯度下降算法的代价是 \(O(N)\) ,运算次数与 \(N\) 成线性关系
</p>
</section>
</section>
</div>
</div>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: './reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: './reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
