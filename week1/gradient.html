<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title></title>
<meta name="author" content="(Ging.wu)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/css/theme/moon.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h2 class="author">Ging.wu</h2><p class="date">Created: 2019-06-18 二 14:47</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org7bbc0cf">1. 全批量梯度下降算法</a>
<ul>
<li><a href="#/slide-orgee26b1d">1.1. 建立如下最小的二乘线性回归模型</a></li>
<li><a href="#/slide-org26e3242">1.2. 最小二乘法的损失函数 \(L(\beta)\) 为</a></li>
<li><a href="#/slide-orge69a0b4">1.3. 损失函数的 \(L(\beta)\) 的梯度</a></li>
<li><a href="#/slide-org04d9f6d">1.4. 梯度下降法的步骤</a>
<ul>
<li><a href="#/slide-orgfb8942e">1.4.1. 当 \(i=0\),自己设置初始点 \(\beta^{0}=\left(\beta_{0}^{0}, \beta_{1}^{0}\right)\) ， 设置步长（也就是学习率 \(\alpha\) ), 设置迭代终止的误差忍耐度 \(tol\) 。</a></li>
<li><a href="#/slide-org8a9acbd">1.4.2. 计算代价函数 \(L(\beta)\) 在点 \((\beta_{0}^{i}, \beta_{1}^{i})\) 上的梯度 \(\nabla L_{\beta^{i}}\)</a></li>
<li><a href="#/slide-org4b8a103">1.4.3. 计算 \(\beta^{i+1}\) , 公式如下</a></li>
<li><a href="#/slide-org0516b6b">1.4.4. 计算梯度 \(\nabla L_{\beta^{i+1}}\) , 如果梯度的二范数 \(\left\|\nabla L_{\beta^{i+1}}\right\|_{2}\) 小于等于 \(tol\) ,则迭代停止，最优解的取值为 \(\beta^{i+1}\) ;如果它大于 \(tol\), 那么 \(i=i+1\) , 并返回第三步。</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#/slide-org972b874">2. 随机梯度下降算法</a>
<ul>
<li><a href="#/slide-orgbf94e50">2.1. 回顾全批量梯度下降算法如何计算每次迭代中的梯度</a></li>
<li><a href="#/slide-orgc4c3bc5">2.2. SGD在计算 \(\nabla L\) 时，并不使用全部样本，而是随机地挑选了一个样本 \(\left(\boldsymbol{x}_{r}, \hat{\boldsymbol{y}}_{r}\right)\)</a></li>
</ul>
</li>
<li><a href="#/slide-org1af4940">3. 小批量随机梯度下降算法</a></li>
</ul>
</div>
</div>
</section>
<section>
<section id="slide-sec-1">
<h2 id="org7bbc0cf"><span class="section-number-2">1</span> 全批量梯度下降算法</h2>
<div class="outline-text-2" id="text-1">
</div>
</section>
<section id="slide-sec-1-1">
<h3 id="orgee26b1d"><span class="section-number-3">1.1</span> 建立如下最小的二乘线性回归模型</h3>
<p>
\(y=\beta_{0}+\beta_{1} x\)
</p>
</section>
<section id="slide-sec-1-2">
<h3 id="org26e3242"><span class="section-number-3">1.2</span> 最小二乘法的损失函数 \(L(\beta)\) 为</h3>
<p>
\(L(\beta)=\frac{1}{N} \sum_{j=1}^{N}\left(y_{j}-\hat{y}_{j}\right)^{2}=\sum_{j=1}^{N} \frac{1}{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)^{2}\)
</p>
</section>
<section id="slide-sec-1-3">
<h3 id="orge69a0b4"><span class="section-number-3">1.3</span> 损失函数的 \(L(\beta)\) 的梯度</h3>
<p>
\(\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(\frac{2}{N} \sum_{j=1}^{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right), \frac{2}{N} \sum_{j=1}^{N} x_{j}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)\right)\)
</p>
</section>
<section id="slide-sec-1-4">
<h3 id="org04d9f6d"><span class="section-number-3">1.4</span> 梯度下降法的步骤</h3>
<div class="outline-text-3" id="text-1-4">
</div>
</section>
<section id="slide-sec-1-4-1">
<h4 id="orgfb8942e"><span class="section-number-4">1.4.1</span> 当 \(i=0\),自己设置初始点 \(\beta^{0}=\left(\beta_{0}^{0}, \beta_{1}^{0}\right)\) ， 设置步长（也就是学习率 \(\alpha\) ), 设置迭代终止的误差忍耐度 \(tol\) 。</h4>
</section>
<section id="slide-sec-1-4-2">
<h4 id="org8a9acbd"><span class="section-number-4">1.4.2</span> 计算代价函数 \(L(\beta)\) 在点 \((\beta_{0}^{i}, \beta_{1}^{i})\) 上的梯度 \(\nabla L_{\beta^{i}}\)</h4>
</section>
<section id="slide-sec-1-4-3">
<h4 id="org4b8a103"><span class="section-number-4">1.4.3</span> 计算 \(\beta^{i+1}\) , 公式如下</h4>
<p>
\(\beta^{i+1}=\beta^{i}-\alpha \nabla L_{\beta^{i}}\)
</p>
</section>
<section id="slide-sec-1-4-4">
<h4 id="org0516b6b"><span class="section-number-4">1.4.4</span> 计算梯度 \(\nabla L_{\beta^{i+1}}\) , 如果梯度的二范数 \(\left\|\nabla L_{\beta^{i+1}}\right\|_{2}\) 小于等于 \(tol\) ,则迭代停止，最优解的取值为 \(\beta^{i+1}\) ;如果它大于 \(tol\), 那么 \(i=i+1\) , 并返回第三步。</h4>


</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="org972b874"><span class="section-number-2">2</span> 随机梯度下降算法</h2>
<p>
  随机梯度下降算法(Stochastic Gradient Decent, SGD) 是对全批量梯度下降算法计算效率的改进算法。本质上来说，我们预期随机梯度下降法得到的
结果和全批量梯度下降法相接近; SGD的优势是更快地计算梯度
</p>
</section>
<section id="slide-sec-2-1">
<h3 id="orgbf94e50"><span class="section-number-3">2.1</span> 回顾全批量梯度下降算法如何计算每次迭代中的梯度</h3>
<p>
   \[\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(\frac{2}{N} \sum_{j=1}^{N}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right), \frac{2}{N} \sum_{j=1}^{N} x_{j}\left(\beta_{0}+\beta_{1} x_{j}-\hat{y}_{j}\right)\right)
\]
全批量梯度下降算法的代价是 \(O(N)\) ,运算次数与 \(N\) 成线性关系, 而随机梯度下降法能将计算一次梯度的代价降低到 \(O(1)\) ,也就是运算次数为常数次，与 \(N\) 无关。所以SGD特别适合大训练样本的计算。
</p>
</section>
<section id="slide-sec-2-2">
<h3 id="orgc4c3bc5"><span class="section-number-3">2.2</span> SGD在计算 \(\nabla L\) 时，并不使用全部样本，而是随机地挑选了一个样本 \(\left(\boldsymbol{x}_{r}, \hat{\boldsymbol{y}}_{r}\right)\)</h3>
<p>
\[\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(2\left(\beta_{0}+\beta_{1} x_{r}-\hat{y}_{r}\right), 2 x_{r}\left(\beta_{0}+\beta_{1} x_{r}-\hat{y}_{r}\right)\right)\]
</p>

</section>
</section>
<section>
<section id="slide-sec-3">
<h2 id="org1af4940"><span class="section-number-2">3</span> 小批量随机梯度下降算法</h2>
<p>
小批量随机梯度下降算法(Mini-batch Stochastic Gradient Decent)是对速度和稳定性进行妥协后的产物。小批量随机梯度下降算法的关键思想是，我们不是随机使用一个样本，而是随机使用 \(b\) 个不同的样本。梯度的计算如下：
\[\nabla L=\left(\frac{\partial L}{\partial \beta_{0}}, \frac{\partial L}{\partial \beta_{1}}\right)=\left(\frac{2}{b} \sum_{r=1}^{b}\left(\beta_{0}+\beta_{1} x_{j_{r}}-\hat{y}_{j_{r}}\right), \frac{2}{b} \sum_{r=1}^{b} x_{j_{r}}\left(\beta_{0}+\beta_{1} x_{j_{r}}-\hat{y}_{j_{r}}\right)\right)\]
</p>
</section>
</section>
</div>
</div>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: './reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: './reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
